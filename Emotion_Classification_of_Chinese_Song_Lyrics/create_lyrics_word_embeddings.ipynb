{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import pandas as pd"],"metadata":{"id":"IbjGS1BYW8dt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install tensorflow"],"metadata":{"id":"g8TtdBt6ZfiX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/CL_final/data/tokenized_rest_lyrics_for_embeddings.csv')\n","\n","# Preprocess data and create vocabulary\n","lyrics_data = df['clean'].astype(str)  # Convert to string type\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lyrics_data)\n","vocabulary_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n","print('start')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kuv7t7WIDJUn","executionInfo":{"status":"ok","timestamp":1685367771673,"user_tz":-480,"elapsed":48002,"user":{"displayName":"標云","userId":"00453514157715556185"}},"outputId":"4c224ad4-3974-4ada-d899-82204ab072da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["start\n"]}]},{"cell_type":"code","source":["vocabulary_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LPYDawiPIzHQ","executionInfo":{"status":"ok","timestamp":1685367771674,"user_tz":-480,"elapsed":51,"user":{"displayName":"標云","userId":"00453514157715556185"}},"outputId":"797b3b3e-2eab-449b-9de6-f3be0690c4df"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["288380"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Access the word index\n","word_index = tokenizer.word_index\n","print(\"Word Index:\")\n","word_index\n","\n","# # Access the word counts\n","# word_counts = tokenizer.word_counts\n","# print(\"\\nWord Counts:\")\n","# print(word_counts)"],"metadata":{"id":"gcLQibCDIb_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate training data\n","window_size = 2\n","training_data = []\n","for lyrics in tqdm(lyrics_data):\n","  word_sequence = tokenizer.texts_to_sequences([lyrics])[0]\n","  for i in range(window_size, len(word_sequence) - window_size):\n","    target_word = word_sequence[i]\n","    context_words = word_sequence[i - window_size: i] + word_sequence[i + 1: i + window_size + 1]\n","    training_data.append((context_words, target_word))\n","\n","# Prepare input and output data\n","X = []\n","y = []\n","for context_words, target_word in training_data:\n","  X.append(context_words)\n","  y.append(target_word)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBFzC4B3XbCt","outputId":"7aa52c5d-374f-49b7-f2dc-afd5c8f5237e","executionInfo":{"status":"ok","timestamp":1685367860588,"user_tz":-480,"elapsed":88954,"user":{"displayName":"標云","userId":"00453514157715556185"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 115636/115636 [01:24<00:00, 1368.69it/s]\n"]}]},{"cell_type":"code","source":["X = np.array(X)\n","y = np.array(y)\n","\n","# Define and train the CBOW model\n","embedding_dim = 300  # Dimensionality of word embeddings\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=window_size*2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(vocabulary_size, activation='softmax')\n","])\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n","model.fit(X, y, epochs=1, batch_size=512)\n","\n","\n","# Extract word embeddings\n","word_embeddings = model.layers[0].get_weights()[0]\n","\n","# Save word embeddings as .txt file\n","with open('word.txt', 'w', encoding='utf-8') as f:\n","  for word, embedding in zip(tokenizer.word_index.keys(), word_embeddings):\n","    embedding_str = ' '.join(str(val) for val in embedding)\n","    f.write(f'{word} {embedding_str}\\n')"],"metadata":{"id":"B2ZqkKwJLQT-"},"execution_count":null,"outputs":[]}]}